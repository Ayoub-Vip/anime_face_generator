{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec01796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.utils.data as tdata\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4980dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-10 10:26:13.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36manime_face_generator.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/ayoubvip/anime_face_generator\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from anime_face_generator.config import PROCESSED_DATA_DIR, RAW_DATA_DIR, MODELS_DIR\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c30ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"number of GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db20652",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27817d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 6\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "IMG_DIM = IMG_SIZE[0] * IMG_SIZE[1] * IMG_CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7036c8",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58a0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000  # total diffusion steps\n",
    "BETA_START = 1e-4\n",
    "BETA_END = 0.02\n",
    "\n",
    "# Linear noise schedule\n",
    "beta = torch.linspace(BETA_START, BETA_END, T, device=device)       # β_t ∈ (0.0001, 0.02)\n",
    "alpha = 1. - beta                           # α_t = 1 - β_t\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)     # \\bar{α}_t = product of α_1 to α_t\n",
    "\n",
    "# model_params:\n",
    "model_config = {\n",
    "  \"im_channels\" : IMG_CHANNELS,\n",
    "  \"im_size\" : IMG_SIZE[0],\n",
    "  \"down_channels\" : [128, 256, 256, 512],\n",
    "  \"mid_channels\" : [512, 512, 256],\n",
    "  # \"down_channels\" : [32, 64, 128, 256, 512],\n",
    "  # \"mid_channels\" : [512, 512, 256],\n",
    "  \"down_sample\" : [True, False, True],\n",
    "  \"time_emb_dim\" : 128,\n",
    "  \"num_down_layers\" : 5,\n",
    "  \"num_mid_layers\" : 3,\n",
    "  \"num_up_layers\" : 5,\n",
    "  \"num_heads\" : 4,\n",
    "  \"dropout_rate\" : 0.1,\n",
    "  \"down_apply_attention\" : [False, True, False, True, True],\n",
    "  \"mid_apply_attention\" : [False, True, False],\n",
    "  \"up_apply_attention\" : [False, True, False, True, False],\n",
    "}\n",
    "\n",
    "# Checking dimension consistency\n",
    "assert model_config[\"mid_channels\"][0] == model_config[\"down_channels\"][-1]\n",
    "assert model_config[\"mid_channels\"][-1] == model_config[\"down_channels\"][-2]\n",
    "assert len(model_config[\"down_apply_attention\"]) == model_config[\"num_down_layers\"]\n",
    "assert len(model_config[\"mid_apply_attention\"]) == model_config[\"num_mid_layers\"]\n",
    "assert len(model_config[\"up_apply_attention\"]) == model_config[\"num_up_layers\"]\n",
    "\n",
    "# Training parameters\n",
    "train_config = {\n",
    "    \"num_epochs\": 20,\n",
    "    \"batch_size\": 6, # Adjusted for GPU memory\n",
    "    \"num_workers\": 4,\n",
    "    \"learning_rate\": 2e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "771623ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, temb_dim: torch.Tensor, device=device) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Convert time steps tensor into an embedding using the\n",
    "    sinusoidal time embedding formula\n",
    "    :param time_steps: 1D tensor of length batch size\n",
    "    :param temb_dim: Dimension of the embedding\n",
    "    :return: BxD embedding representation of B time steps\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "    half_dim = temb_dim // 2\n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** ((torch.arange(start=0, end=half_dim, dtype=torch.float32, device=device) / half_dim)\n",
    "    )\n",
    "    \n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, half_dim) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels=64, num_heads=4, batch_first=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups=8, num_channels=channels)\n",
    "        self.mhsa = nn.MultiheadAttention(embed_dim=self.channels, num_heads=num_heads, batch_first=batch_first)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        h = h.reshape(B, self.channels, H * W).swapaxes(1, 2)  # [B, C, H, W] --> [B, C, H * W] --> [B, H*W, C]\n",
    "        h, _ = self.mhsa(h, h, h)  # [B, H*W, C]\n",
    "        h = h.swapaxes(2, 1).view(B, self.channels, H, W)  # [B, C, H*W] --> [B, C, H, W]\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class DownEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample using 2x2 average pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample=True, num_heads=4, num_layers=1, dropout_rate=0.1, apply_attention=[False, False, True, False]):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        \n",
    "        assert len(apply_attention) == num_layers, \"apply_attention must have the same length as num_layers\"\n",
    "        \n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout2d(p=dropout_rate),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.attention = nn.ModuleList(\n",
    "            [AttentionBlock(channels=out_channels, num_heads=num_heads, batch_first=True) if apply_attention[i] else nn.Identity()\n",
    "             for i in range(num_layers)]\n",
    "        )\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            # Attention block of Unet\n",
    "            out = self.attention[i](out)\n",
    "            \n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1, dropout_rate=0.1, apply_attention=[False, True, False]):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        assert len(apply_attention) == num_layers, \"apply_attention must have the same length as num_layers\"\n",
    "        \n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers + 1)\n",
    "        ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout2d(p=dropout_rate),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers+1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.ModuleList(\n",
    "            [AttentionBlock(channels=out_channels, num_heads=num_heads, batch_first=True) if apply_attention[i] else nn.Identity()\n",
    "             for i in range(num_layers)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        \n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            # Attention Block\n",
    "            out = self.attention[i](out)\n",
    "            \n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i+1](out)\n",
    "            out = out + self.t_emb_layers[i+1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i+1](out)\n",
    "            out = out + self.residual_input_conv[i+1](resnet_input)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class UpDecoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1, dropout_rate=0.1, apply_attention=[False, False, True, False]):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        \n",
    "        assert len(apply_attention) == num_layers, \"apply_attention must have the same length as num_layers\"\n",
    "        \n",
    "        \n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.t_emb_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(t_emb_dim, out_channels)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(8, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Dropout2d(p=dropout_rate),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.ModuleList(\n",
    "            [AttentionBlock(channels=out_channels, num_heads=num_heads, batch_first=True) if apply_attention[i] else nn.Identity()\n",
    "             for i in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
    "                                                 4, 2, 1) \\\n",
    "            if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down, t_emb):\n",
    "        x = self.up_sample_conv(x)\n",
    "        x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            # Attention block\n",
    "            out = self.attention[i](out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Unet model comprising\n",
    "    Down blocks, Midblocks and Uplocks\n",
    "    \"\"\"\n",
    "    def __init__(self, model_config):\n",
    "        super().__init__()\n",
    "        im_channels = model_config['im_channels']\n",
    "        self.down_channels = model_config['down_channels']\n",
    "        self.mid_channels = model_config['mid_channels']\n",
    "        self.t_emb_dim = model_config['time_emb_dim']\n",
    "        self.down_sample = model_config['down_sample']\n",
    "        self.num_down_layers = model_config['num_down_layers']\n",
    "        self.num_mid_layers = model_config['num_mid_layers']\n",
    "        self.num_up_layers = model_config['num_up_layers']\n",
    "        self.num_heads = model_config['num_heads']\n",
    "        self.dropout_rate = model_config['dropout_rate']\n",
    "        \n",
    "        assert self.mid_channels[0] == self.down_channels[-1]\n",
    "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
    "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
    "\n",
    "        # Initial projection from sinusoidal time embedding\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
    "        )\n",
    "\n",
    "        self.up_sample = list(reversed(self.down_sample))\n",
    "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n",
    "\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels)-1):\n",
    "            self.downs.append(DownEncoder(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim,\n",
    "                                        down_sample=self.down_sample[i], num_heads=self.num_heads, num_layers=self.num_down_layers, dropout_rate=self.dropout_rate,\n",
    "                                        apply_attention=model_config['down_apply_attention']))\n",
    "        \n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels)-1):\n",
    "            self.mids.append(BottleNeck(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim,\n",
    "                                      num_heads=self.num_heads, num_layers=self.num_mid_layers, dropout_rate=self.dropout_rate,\n",
    "                                      apply_attention=model_config['mid_apply_attention']))\n",
    "        \n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels)-1)):\n",
    "            self.ups.append(UpDecoder(self.down_channels[i] * 2, self.down_channels[i-1] if i != 0 else 16,\n",
    "                                    self.t_emb_dim, up_sample=self.down_sample[i], num_heads=self.num_heads, num_layers=self.num_up_layers, dropout_rate=self.dropout_rate,\n",
    "                                    apply_attention=model_config['up_apply_attention']))\n",
    "        \n",
    "        self.norm_out = nn.GroupNorm(8, 16)\n",
    "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
    "        # Shapes assuming midblocks are [C4, C4, C3]\n",
    "        # Shapes assuming downsamples are [True, True, False]\n",
    "        # B x C x H x W\n",
    "        out = self.conv_in(x)\n",
    "        # B x C1 x H x W\n",
    "        \n",
    "        # t_emb -> B x t_emb_dim\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t, device=x.device).long(), self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        \n",
    "        down_outs = []\n",
    "        \n",
    "        for idx, down in enumerate(self.downs):\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
    "        # out B x C4 x H/4 x W/4\n",
    "        \n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        # out B x C3 x H/4 x W/4\n",
    "        \n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        # out B x C x H x W\n",
    "        return out\n",
    "    \n",
    "def q_sample(x0, t, noise=None): # add noise | forward pass | q(x_t | x_t-1)\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x0).to(device)\n",
    "    \n",
    "    sqrt_alpha_bar = alpha_bar[t].sqrt().view(-1, 1, 1, 1)\n",
    "    sqrt_one_minus_alpha_bar = (1 - alpha_bar[t]).sqrt().view(-1, 1, 1, 1)\n",
    "    \n",
    "    return sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise # <= x_t \n",
    "\n",
    "def get_loss(model, x0, t=None):\n",
    "    B = x0.shape[0]\n",
    "    if t is None:\n",
    "        t = torch.randint(0, T, (B,), device=x0.device).long()\n",
    "    noise = torch.randn_like(x0)\n",
    "    x_t = q_sample(x0, t, noise)\n",
    "    pred_noise = model(x_t, t)\n",
    "    return nn.MSELoss()(pred_noise, noise), pred_noise, x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b58f65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Unet(model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd238185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 84.96M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "model = Unet(model_config)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "if device.type == 'cuda':\n",
    "    model = model.cuda()\n",
    "\n",
    "# os.environ[\"NCCL_DEBUG\"]=\"INFO\"\n",
    "num_model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_model_params / 1e6:.2f}M\")\n",
    "backup_model = \"85M_params_GIGA_DDPM_Unet_ckpt_epoch_1.pth\"\n",
    "# \"74M_params_ddpm_Unet_ckpt_epoch_1.pth\"\n",
    "# \"48M_param_ddpm_Unet_ckpt_epoch_1.pth\"\n",
    "# \"2025-08-04_21-09-16_VDM_huge_tailored_advanced_UNET_model--good-denoising-medium-quality-generalization-1.07-epochs.pth\"\n",
    "# \"40MB_ddpm_Unet_ckpt_epoch_4.pth\"\n",
    "# \"good-denoising-bad-generalization--2025-08-02_20-59-28_VDM_advanced_UNET_model.pth\"\n",
    "state= torch.load(MODELS_DIR / backup_model, weights_only=True, map_location=device)\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f5f84",
   "metadata": {},
   "source": [
    "## Simpling from the learned distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3592ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 1000  # total diffusion steps\n",
    "# BETA_START = 0\n",
    "# BETA_END = T\n",
    "# DELTA_TIME = T + 1\n",
    "\n",
    "# # --- Noise schedule (cosine) ---\n",
    "# def cosine_beta_schedule(s=0.008):\n",
    "#     x = torch.linspace(BETA_START, T, DELTA_TIME, device=device)\n",
    "#     alphas_cumprod = torch.cos(((x / T) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
    "#     alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "#     betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "#     return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "# # Cosine noise schedule\n",
    "# beta = cosine_beta_schedule(s=0.008)       # β_t ∈ (0.0001, 0.02)\n",
    "# alpha = 1. - beta                           # α_t = 1 - β_t\n",
    "# alpha_bar = torch.cumprod(alpha, dim=0)     # \\bar{α}_t = product of α_1 to α_t\n",
    "# sqrt_one_minus_alphas_bar = torch.sqrt(1. - alpha_bar)  # \\sqrt{1 - \\bar{α}_t}\n",
    "# sqrt_alphas_bar = torch.sqrt(alpha_bar)  # \\sqrt{\\bar{α}_t}\n",
    "\n",
    "# def p_sample(model, shape, steps=50):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         betas = cosine_beta_schedule(T).to(device)\n",
    "#         alphas = 1 - betas\n",
    "#         alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "#         x = torch.randn(shape, device=device)\n",
    "#         t_array = torch.linspace(T - 1, 0, steps, device=device).long()\n",
    "#         for i in range(steps):\n",
    "#             t = t_array[i].unsqueeze(0)\n",
    "#             for _ in range(1):\n",
    "#                 grad = model(x, t)\n",
    "#                 x = x + 0.01 * grad + torch.sqrt(torch.tensor(2 * 0.01)).to(device) * torch.randn_like(x)\n",
    "#             beta_t = betas[t]\n",
    "#             alpha_t = alphas[t]\n",
    "#             drift = (1 - alpha_t).sqrt() * model(x, t)\n",
    "#             diffusion = beta_t.sqrt() * torch.randn_like(x)\n",
    "#             x = x + drift + diffusion\n",
    "#         return x\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x_t, t):\n",
    "    beta_t = beta[t].view(-1, 1, 1, 1)\n",
    "    sqrt_alpha_t = alpha[t].sqrt().view(-1, 1, 1, 1)\n",
    "    sqrt_one_minus_alpha_bar_t = (1 - alpha_bar[t]).sqrt().view(-1, 1, 1, 1)\n",
    "\n",
    "    # Predict noise\n",
    "    eps_theta = model(x_t, t)\n",
    "\n",
    "    # Compute predicted x_0\n",
    "    pred_x0 = (x_t - sqrt_one_minus_alpha_bar_t * eps_theta) / sqrt_alpha_t\n",
    "\n",
    "    # Compute mean (μ_θ)\n",
    "    mu = (1 / sqrt_alpha_t) * (x_t - beta_t / sqrt_one_minus_alpha_bar_t * eps_theta)\n",
    "\n",
    "    # Sample noise (except for last step)\n",
    "    if t[0] > 0:\n",
    "        noise = torch.randn_like(x_t)\n",
    "    else:\n",
    "        noise = torch.zeros_like(x_t)\n",
    "\n",
    "    x_prev = mu + beta_t.sqrt() * noise\n",
    "    return x_prev.clamp(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ad16387",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, shape=(1, 1, 64, 64), step=50):\n",
    "    x = torch.randn(shape).to(device)\n",
    "    B = shape[0]    #batch size, number of samples to generate\n",
    "    # Generate samples by reversing the diffusion process\n",
    "    for t_step in reversed(range(T)):\n",
    "        t = torch.full((B,), t_step, dtype=torch.long).to(device)\n",
    "        x = p_sample(model, x, t)\n",
    "    # x = p_sample(model, shape, step)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a297d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples(model, num_samples=48, image_size=IMG_SIZE, step=50):\n",
    "    model.eval()\n",
    "    samples = generate(model, shape=(num_samples, IMG_CHANNELS, *image_size), step=step)\n",
    "    return samples\n",
    "\n",
    "def show_images_grid_rgb(samples, nrow=8, ncol=6):\n",
    "    fig, axes = plt.subplots(ncol, nrow, figsize=(nrow, ncol))\n",
    "    samples = (samples + 1) / 2         # Rescale from [-1, 1] to [0, 1]\n",
    "    samples = samples.clamp(0, 1).cpu() # Ensure valid pixel range\n",
    "    samples = samples.permute(0, 2, 3, 1)  # Convert to (N, H, W, C) for plotting\n",
    "\n",
    "    idx = 0\n",
    "    for row in range(ncol):\n",
    "        for col in range(nrow):\n",
    "            if idx == samples.shape[0]:\n",
    "                break\n",
    "            axes[row, col].imshow(samples[idx])\n",
    "            axes[row, col].axis('off')\n",
    "            idx += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "samples = generate_samples(model, num_samples=2, image_size=IMG_SIZE, step=20)  # adapt shape to your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_images_grid_rgb(samples, nrow=4, ncol=2) #epoch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m             x_t \u001b[38;5;241m=\u001b[39m mean  \u001b[38;5;66;03m# final prediction\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x_t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# rescale from [-1,1] to [0,1]\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m samples \u001b[38;5;241m=\u001b[39m generate_samples(model, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m, image_size\u001b[38;5;241m=\u001b[39mIMG_SIZE)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36mgenerate_samples\u001b[0;34m(model, num_samples, image_size, step)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_samples\u001b[39m(model, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m, image_size\u001b[38;5;241m=\u001b[39mIMG_SIZE, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m     samples \u001b[38;5;241m=\u001b[39m generate(model, shape\u001b[38;5;241m=\u001b[39m(num_samples, IMG_CHANNELS, \u001b[38;5;241m*\u001b[39mimage_size), step\u001b[38;5;241m=\u001b[39mstep)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, shape, step)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(T)):\n\u001b[1;32m      7\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((B,), t_step, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m p_sample(model, x, t)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# x = p_sample(model, shape, step)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m, in \u001b[0;36mp_sample\u001b[0;34m(model, x_t, t)\u001b[0m\n\u001b[1;32m     44\u001b[0m sqrt_one_minus_alpha_bar_t \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_bar[t])\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Predict noise\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m eps_theta \u001b[38;5;241m=\u001b[39m model(x_t, t)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Compute predicted x_0\u001b[39;00m\n\u001b[1;32m     50\u001b[0m pred_x0 \u001b[38;5;241m=\u001b[39m (x_t \u001b[38;5;241m-\u001b[39m sqrt_one_minus_alpha_bar_t \u001b[38;5;241m*\u001b[39m eps_theta) \u001b[38;5;241m/\u001b[39m sqrt_alpha_t\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 353\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m up \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mups:\n\u001b[1;32m    352\u001b[0m     down_out \u001b[38;5;241m=\u001b[39m down_outs\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m--> 353\u001b[0m     out \u001b[38;5;241m=\u001b[39m up(out, down_out, t_emb)\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\u001b[39;00m\n\u001b[1;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_out(out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 270\u001b[0m, in \u001b[0;36mUpDecoder.forward\u001b[0;34m(self, x, out_down, t_emb)\u001b[0m\n\u001b[1;32m    267\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_input_conv[i](resnet_input)\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# Attention block\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention[i](out)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_norm(x)\n\u001b[1;32m     32\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels, H \u001b[38;5;241m*\u001b[39m W)\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [B, C, H, W] --> [B, C, H * W] --> [B, H*W, C]\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m h, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmhsa(h, h, h)  \u001b[38;5;66;03m# [B, H*W, C]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mswapaxes(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels, H, W)  \u001b[38;5;66;03m# [B, C, H*W] --> [B, C, H, W]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m h\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py:1308\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_masks(\n\u001b[1;32m   1304\u001b[0m             attn_mask, key_padding_mask, query\n\u001b[1;32m   1305\u001b[0m         )\n\u001b[1;32m   1307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1308\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_native_multi_head_attention(\n\u001b[1;32m   1309\u001b[0m                 query,\n\u001b[1;32m   1310\u001b[0m                 key,\n\u001b[1;32m   1311\u001b[0m                 value,\n\u001b[1;32m   1312\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m   1313\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1314\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight,\n\u001b[1;32m   1315\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1316\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1317\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1318\u001b[0m                 merged_mask,\n\u001b[1;32m   1319\u001b[0m                 need_weights,\n\u001b[1;32m   1320\u001b[0m                 average_attn_weights,\n\u001b[1;32m   1321\u001b[0m                 mask_type,\n\u001b[1;32m   1322\u001b[0m             )\n\u001b[1;32m   1324\u001b[0m any_nested \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mor\u001b[39;00m key\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mor\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_nested\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m any_nested, (\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiheadAttention does not support NestedTensor outside of its fast path. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe fast path was not hit because \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwhy_not_fast_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1328\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def sample_ddpm(model, n_samples, T, IMG_SIZE=(3, 64, 64)):\n",
    "    model.eval()\n",
    "    x_t = torch.randn(n_samples, *IMG_SIZE).to(device) #+ ((samples * 2) - 1) #TODO:remove \n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        t_tensor = torch.full((n_samples,), t, dtype=torch.long, device=device)\n",
    "\n",
    "        beta_t = beta[t]\n",
    "        alpha_t = alpha[t]\n",
    "        alpha_bar_t = alpha_bar[t]\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar_t)\n",
    "        sqrt_recip_alpha = torch.sqrt(1.0 / alpha_t)\n",
    "\n",
    "        eps_theta = model(x_t, t_tensor)\n",
    "\n",
    "        # Predict x0\n",
    "        x0_pred = (x_t - sqrt_one_minus_alpha_bar * eps_theta) / torch.sqrt(alpha_bar_t)\n",
    "\n",
    "        # Predict mean for x_{t-1}\n",
    "        mean = sqrt_recip_alpha * (x_t - beta_t / sqrt_one_minus_alpha_bar * eps_theta)\n",
    "\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x_t)\n",
    "            sigma = torch.sqrt(beta_t)\n",
    "            x_t = mean + sigma * noise\n",
    "        else:\n",
    "            x_t = mean  # final prediction\n",
    "\n",
    "    return (x_t + 1) / 2  # rescale from [-1,1] to [0,1]\n",
    "\n",
    "samples = generate_samples(model, num_samples=18, image_size=IMG_SIZE)  # adapt shape to your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_grid_rgb(samples, nrow=6, ncol=3)\n",
    "# sample_ddpm(model, n_samples, T,IMG_SIZE) epochs 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# label = \"good-denoising-medium-quality-generalization-2.1-epochs\"\n",
    "# model_name = time.strftime(\"%Y-%m-%d_%H-%M-%S\") + f\"_VDM_huge_tailored_advanced_UNET_model--{label}-\" + \".pth\"\n",
    "# torch.save(model.state_dict(), MODELS_DIR / model_name)\n",
    "# print(\"Saved PyTorch Model State to \" +  model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Saving the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# IPython.notebook.save_notebook()\n",
    "from ipylab import JupyterFrontEnd\n",
    "app = JupyterFrontEnd()\n",
    "app.commands.execute('docmanager:save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
